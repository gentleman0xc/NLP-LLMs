{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Codificando Mecanismos de atenção\n",
        "\n",
        "* Explorando as razões para usar mecanismos de atenção em redes neurais\n",
        "\n",
        "* Apresentando uma estrutura básica de autoatenção e progredindo para um mecanismo aprimorado de autoatenção\n",
        "\n",
        "* Implementando um módulo de atenção causal que permite que LLMs gerem um token por vez\n",
        "\n",
        "* Mascarando pesos de atenção selecionados aleatoriamente com abandono para reduzir o overfitting\n",
        "\n",
        "* Empilhar vários módulos de atenção causal em um módulo de atenção com várias cabeças\n",
        "\n",
        "<img src=\"https://drek4537l1klr.cloudfront.net/raschka/v-4/Figures/03__image013.png\">"
      ],
      "metadata": {
        "id": "HprK3amsJ0Af"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considere o texto:\n",
        "\"Your journey starts with one step.\"\n",
        "\n",
        "Aqui esses vetores são representados como tridimensionais.\n",
        "O modelo GPT2 possui 768 dimensões para cada token (palavra ou subpalavra)"
      ],
      "metadata": {
        "id": "JZfKkClGPaGq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDOiQgGlJvRz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O objetivo geral desta seção é ilustrar o cálculo do vetor de contexto z (2) usando a segunda sequência de entrada, x (2) como uma consulta. Esta figura mostra a primeira etapa intermediária, calculando as pontuações de atenção ω entre a consulta x (2) e todos os outros elementos de entrada como um produto escalar.\n",
        "\n",
        "<img src=\"https://drek4537l1klr.cloudfront.net/raschka/v-4/Figures/03__image015.png\">"
      ],
      "metadata": {
        "id": "ipTWghIDPyfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs.shape[0]) # quantidade de linhas\n",
        "print(inputs.shape[1]) # quantidade de colunas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_UTHY8S8X87",
        "outputId": "1d41f28a-ce27-4abd-85af-1968b6171538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t1rE4lgxDWst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = inputs[1] # O segundo token de entrada serve como consulta (query)\n",
        "attn_scores_2 = torch.empty(inputs.shape[0]) # cria um tensor vazio com o mesmo tamanho do numero de vetores de entrada\n",
        "for i, x_i in enumerate(inputs):\n",
        "  attn_scores_2[i] = torch.dot(x_i, query) # Para cada query definida calculamos o produto escalar entre esse vetor da query e cada outro vetor de entrada\n",
        "\n",
        "print(attn_scores_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbaZPyT-8gjZ",
        "outputId": "bc0f4cf3-5f6d-496b-ebef-b086ccc5a252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# entendendo o i e x_i dos inputs\n",
        "for i, x_i in enumerate(inputs):\n",
        "  print(i, '------->', x_i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB2-Ko2sXQt2",
        "outputId": "4a7a8226-a10b-40fe-d7f0-7b50a604a251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 -------> tensor([0.4300, 0.1500, 0.8900])\n",
            "1 -------> tensor([0.5500, 0.8700, 0.6600])\n",
            "2 -------> tensor([0.5700, 0.8500, 0.6400])\n",
            "3 -------> tensor([0.2200, 0.5800, 0.3300])\n",
            "4 -------> tensor([0.7700, 0.2500, 0.1000])\n",
            "5 -------> tensor([0.0500, 0.8000, 0.5500])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compreendendo os Produtos Escalares\n",
        "\n",
        "é uma maneira de multiplicarmos dois vetores elemento a elemento e depois somar os produtos"
      ],
      "metadata": {
        "id": "j-BZyyp3SSmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = 0.\n",
        "for idx, element in enumerate(inputs[0]):\n",
        "  res += inputs[0][idx] * query[idx]\n",
        "print(res)\n",
        "print(torch.dot(inputs[0], query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNwKunZlbH0D",
        "outputId": "aef58b55-c2d3-4033-de72-0a8a132c2307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.9544)\n",
            "tensor(0.9544)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ohQsLKK4SM9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depois de calcular os scores de atenção W21 a W2T em relação á query de entrada x(2), o proximo passo é obter os pesos de atenção\n",
        "a21 a a2T  normalizando os scores de atenção\n",
        "\n",
        "<img src=\"https://drek4537l1klr.cloudfront.net/raschka/v-4/Figures/03__image017.png\">\n",
        "\n",
        "O principal objetivo por trás da normalização é obter pesos de atenção que somam 1."
      ],
      "metadata": {
        "id": "pwnDMXH_S12R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
        "print(\"Attention weights: \", attn_weights_2_tmp)\n",
        "print(\"sum: \", attn_weights_2_tmp.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VkJseyC9wDe",
        "outputId": "c1e07ae7-37ce-4f88-aca4-eec5c7986fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights:  tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
            "sum:  tensor(1.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Na prática, é mais comum e aconselhável utilizar a função softmax para normalização. Esta abordagem é melhor no gerenciamento de valores extremos e oferece propriedades de gradiente mais favoráveis ​​durante o treinamento. Abaixo está uma implementação básica da função softmax para normalizar as pontuações de atenção:"
      ],
      "metadata": {
        "id": "PagZk-LoTvoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_naive(x):\n",
        "  return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
        "\n",
        "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
        "print(\"Attention weights:\", attn_weights_2_naive)\n",
        "print(\"Sum: \", attn_weights_2_naive.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkGQuKJfTm9w",
        "outputId": "1cb3d0c4-d86c-46b4-906b-e72af31b9151"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum:  tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta implementação ingenua do softmax pode trazer problemas de instabilidade numerica.\n",
        "\n",
        "Portanto é recomendavel usar a implementação softmax do pytorch\n",
        "\n"
      ],
      "metadata": {
        "id": "3rs8Zh3MUSog"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6XPP7D9kG6yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights_2 = torch.softmax(attn_scores_2, dim=0) #Uma dimensão ao longo da qual o Softmax será calculado\n",
        "print(\"Attention weights: \", attn_weights_2)\n",
        "print(\"Sum: \", attn_weights_2.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_Z_1ICD9_qI",
        "outputId": "f5b7090e-745a-49ef-abb1-dbd388dd6dc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum:  tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A etapa final, após calcular e normalizar os escores de atenção para obter os pesos de atenção para a consulta x (2) , é calcular o vetor de contexto z (2) . Este vetor de contexto é uma combinação de todos os vetores de entrada x (1) a x (T) ponderados pelos pesos de atenção.\n",
        "\n",
        "<img src=\"https://drek4537l1klr.cloudfront.net/raschka/v-4/Figures/03__image019.png\">\n"
      ],
      "metadata": {
        "id": "XinHIRsyYOhG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eBU6loH1IDvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = inputs[1]\n",
        "context_vec_2 = torch.zeros(query.shape) # tensor de zeros, query.shape = 3\n",
        "for i, x_i in enumerate(inputs):\n",
        "  context_vec_2 += attn_weights_2[i] * x_i\n",
        "print(context_vec_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiV6Xemshh0U",
        "outputId": "27d9dcdf-b198-42dd-867c-ca3360e1e2fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculando pesos de atenção para todos os tokens de entrada\n",
        "\n",
        "<img src=\"https://drek4537l1klr.cloudfront.net/raschka/v-4/Figures/03__image023.png\">"
      ],
      "metadata": {
        "id": "WF8HjG1YajGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = torch.empty(6,6)\n",
        "for i, x_i in enumerate(inputs):\n",
        "  for j, x_j in enumerate(inputs):\n",
        "    attn_scores[i, j] = torch.dot(x_i, x_j)\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "id": "DUCnZMXXZ4nI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9bf5b7d-0bf2-4403-a523-312898314315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ao calcular o tensor de pontuação de atenção anterior, usamos loops for em Python. No entanto, os loops for geralmente são lentos e podemos obter os mesmos resultados usando a multiplicação de matrizes:"
      ],
      "metadata": {
        "id": "LwGDMlt8w0DE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = inputs @ inputs.T # multiplicação de uma matriz 6x3 com outra 3x6 resultando em uma matriz 6x6\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQJXIG1tMRfk",
        "outputId": "3b2aec2f-e834-45d6-b90e-c95c8fb5bb69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Na etapa 2, normalizamos cada linha para que os valores em cada linha somem 1"
      ],
      "metadata": {
        "id": "VzVpM0S9w-u3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PH26L21pMaRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(attn_scores, dim=1)\n",
        "print(attn_weights)\n",
        "print(attn_weights[0].sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQzAKtmf_d6Z",
        "outputId": "7cd4b191-aebe-4b60-ee9d-4f4af1cbcce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
            "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
            "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
            "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
            "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
            "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
            "tensor(1.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vamos verificar msm\n",
        "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
        "print(\"Row 2 sum:\", row_2_sum)\n",
        "print(\"All row sums:\", attn_weights.sum(dim=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQF8mRA3xHQT",
        "outputId": "400a0aee-4264-445d-95b2-e550b4f300c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 2 sum: 1.0\n",
            "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gerando essa imagem:\n",
        "\n",
        "<img src=\"https://drek4537l1klr.cloudfront.net/raschka/v-4/Figures/03__image021.png\">\n",
        "\n",
        "A linha destacada mostra os pesos de atenção para o segundo elemento de entrada como uma consulta, conforme calculamos na seção anterior. Esta seção generaliza o cálculo para obter todos os outros pesos de atenção."
      ],
      "metadata": {
        "id": "XO6IJ1qLxb-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Na terceira e ultima etapa, usamos agora esses pesos de atenção para calcular todos os vetores de contexto por meio da multiplicação de matrizes"
      ],
      "metadata": {
        "id": "iIp8QkWzxuU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_context_vecs = attn_weights @ inputs\n",
        "print(all_context_vecs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViUWsV1xABLA",
        "outputId": "9c30fc7e-18a1-4cc1-ae21-93901306168e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4421, 0.5931, 0.5790],\n",
            "        [0.4419, 0.6515, 0.5683],\n",
            "        [0.4431, 0.6496, 0.5671],\n",
            "        [0.4304, 0.6298, 0.5510],\n",
            "        [0.4671, 0.5910, 0.5266],\n",
            "        [0.4177, 0.6503, 0.5645]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# verificando se o codigo está correto comparando context_vec_2 calculado anteriormente\n",
        "print(\"Previous 2nd context vector:\", context_vec_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvOgMEuFx3TU",
        "outputId": "8bfd5bb2-36c6-481b-cead-5c1b57dedd4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Isso conclui o passo a passo do código de um mecanismo simples de autoatenção. Na próxima seção, adicionaremos pesos treináveis, permitindo que o LLM aprenda com os dados e melhore seu desempenho em tarefas específicas."
      ],
      "metadata": {
        "id": "9uvEkqElyHkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementando autoatenção com pesos treináveis\n",
        "\n",
        "### Calculando os pesos de atenção passo a passo\n",
        "\n",
        "<img src=\"https://drek4537l1klr.cloudfront.net/raschka/v-5/Figures/03__image027.png\">"
      ],
      "metadata": {
        "id": "llk6FOlEil1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_2 = inputs[1] # a segunda entrada [0.5500, 0.8700, 0.6600]\n",
        "d_in = inputs.shape[1] # tamanho do embedding de entrada, d = 3\n",
        "d_out = 2 # tamanho do embedding de saída"
      ],
      "metadata": {
        "id": "ZGGxvcqf0I_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializando matrizes de pesos Wq, Wk e Wv\n",
        "torch.manual_seed(123)\n",
        "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
      ],
      "metadata": {
        "id": "EPxMNrdW0WBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculando os vetores query, key and value\n",
        "query_2 = x_2 @ W_query\n",
        "key_2 = x_2 @ W_key\n",
        "value_2 = x_2 @ W_value\n",
        "print(query_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwpJZTLho7ZB",
        "outputId": "3793bafb-1d7c-439b-82f6-9a4b76fc41c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4306, 1.4551])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Isso retorna um vetor bidimensional pois definimos o numero de colunas da matriz de pesos em `d_out=2`"
      ],
      "metadata": {
        "id": "gArFczSSpQ94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embora nosso objetivo temporário seja calcular apenas um vetor de contexto, z (2) , ainda precisamos dos vetores de chave e valor para todos os elementos de entrada, pois eles estão envolvidos no cálculo dos pesos de atenção em relação à consulta q (2) , conforme ilustrado na Figura 3.14.\n",
        "\n",
        "Podemos obter todas as chaves e valores através da multiplicação de matrizes:"
      ],
      "metadata": {
        "id": "na8y7TgKuKRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys = inputs @ W_key\n",
        "values = inputs @ W_value\n",
        "print(\"keys.shape:\", keys.shape)\n",
        "print(\"values.shape: \", values.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYaj1J_HAxVs",
        "outputId": "785b0b8f-8eec-43d3-bda8-b9ee15b42594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keys.shape: torch.Size([6, 2])\n",
            "values.shape:  torch.Size([6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dvPBikivpM8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(keys)\n",
        "print(\"\\n\",values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRYn0sVjujru",
        "outputId": "25fc8047-24b3-4fa3-9ac7-bf6ce850422c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3669, 0.7646],\n",
            "        [0.4433, 1.1419],\n",
            "        [0.4361, 1.1156],\n",
            "        [0.2408, 0.6706],\n",
            "        [0.1827, 0.3292],\n",
            "        [0.3275, 0.9642]])\n",
            "\n",
            " tensor([[0.1855, 0.8812],\n",
            "        [0.3951, 1.0037],\n",
            "        [0.3879, 0.9831],\n",
            "        [0.2393, 0.5493],\n",
            "        [0.1492, 0.3346],\n",
            "        [0.3221, 0.7863]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O segundo passo agora é calcular os escores de atenção\n",
        "\n",
        "**O cálculo da pontuação de atenção é um cálculo de produto escalar semelhante ao que usamos no mecanismo simplificado de autoatenção. O novo aspecto aqui é que não estamos calculando diretamente o produto escalar entre os elementos de entrada, mas usando a consulta e a chave obtidas pela transformação das entradas por meio das respectivas matrizes de pesos.**\n",
        "\n",
        "<img src=\"https://drek4537l1klr.cloudfront.net/raschka/v-5/Figures/03__image029.png\">\n"
      ],
      "metadata": {
        "id": "GdnKx4EVvmc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primeiro vamos calcular a pontuação de atenção w22:\n",
        "\n",
        "obs: esse w não é o de peso (parâmetro)\n"
      ],
      "metadata": {
        "id": "wKS3qzMhwNMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys_2 = keys[1]\n",
        "attn_score_22 = query_2.dot(keys_2)\n",
        "print(attn_score_22)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSf8VlTuu24_",
        "outputId": "6dea926d-b3ad-447d-9af2-cb5383bece70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.8524)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSX2V9ruww0A",
        "outputId": "beceb87d-cd11-4f80-e113-d75129f19f83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.4306, 1.4551])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rcUUoiewziv",
        "outputId": "63a45571-af1a-4842-e8db-0b581af9e6dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3669, 0.4433, 0.4361, 0.2408, 0.1827, 0.3275],\n",
              "        [0.7646, 1.1419, 1.1156, 0.6706, 0.3292, 0.9642]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fl0C73LeIeEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generalizando o calculo para todas as pontuações(scores) de atenção\n",
        "attn_scores_2 = query_2 @ keys.T # Todas as pontuações(scores) de atenção para determinado query\n",
        "print(attn_scores_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fr457Mewwisu",
        "outputId": "2acb7687-709c-4879-ddc4-6cfd1c809a82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Após calcular os escores de atenção ω , o próximo passo é normalizar esses escores usando a função softmax para obter os pesos de atenção α .\n",
        "\n",
        "<img src=\"https://drek4537l1klr.cloudfront.net/raschka/v-5/Figures/03__image031.png\">\n",
        "\n",
        "calculamos os pesos de atenção dimensionando as pontuações de atenção e usando a função softmax que usamos anteriormente. A diferença em relação ao anterior é que agora dimensionamos as pontuações de atenção dividindo-as pela raiz quadrada da incorporação dimensão das chaves, (observe que tirar a raiz quadrada é matematicamente o mesmo que exponenciar por 0,5):"
      ],
      "metadata": {
        "id": "Cl1J23KKxOU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_k = keys.shape[-1]\n",
        "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
        "print(attn_weights_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ldb9HPpBP_w",
        "outputId": "a9852960-ae53-4ed6-f706-ba2a6d83aafe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Na etapa final do cálculo da autoatenção, calculamos o vetor de contexto combinando todos os vetores de valor por meio dos pesos de atenção.\n",
        "\n",
        "<img src=\"https://drek4537l1klr.cloudfront.net/raschka/v-5/Figures/03__image033.png\">"
      ],
      "metadata": {
        "id": "JI1fO1gT3dhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_vec_2 = attn_weights_2 @ values\n",
        "print(context_vec_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ap7PwaFFB3_Z",
        "outputId": "3eea843c-8e35-4989-a3cb-f5c662bf30aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3061, 0.8210])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Até agora, calculamos apenas um unico vetor de contexto, z(2), Iremos generalizar para calcular todos os vetores de contexto na sequencia de entrada."
      ],
      "metadata": {
        "id": "JJw4HDDN341S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementando uma classe Python compacta de autoatenção\n"
      ],
      "metadata": {
        "id": "zrqeCHRcGvFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uma classe compacta de autoatenção\n",
        "\n",
        "import torch.nn as nn\n",
        "class SelfAttention_v1(nn.Module):\n",
        "    def __init__(self, d_in, d_out):\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
        "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
        "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = x @ self.W_key\n",
        "        queries = x @ self.W_query\n",
        "        values = x @ self.W_value\n",
        "        attn_scores = queries @ keys.T # omega\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec\n"
      ],
      "metadata": {
        "id": "a_x4XDcG3yBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Podemos usar a classe da seguinte forma:\n",
        "torch.manual_seed(123)\n",
        "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
        "print(sa_v1(inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUsSB9pCCrHh",
        "outputId": "f706f58a-6c9c-4fc5-84c3-98261a1cf8f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2996, 0.8053],\n",
            "        [0.3061, 0.8210],\n",
            "        [0.3058, 0.8203],\n",
            "        [0.2948, 0.7939],\n",
            "        [0.2927, 0.7891],\n",
            "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_yLV85F9H4Ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como uma verificação rápida, observe como a segunda linha corresponde ao conteúdo do context_vec_2 na seção anterior.\n"
      ],
      "metadata": {
        "id": "sYu0v4pKIy1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drek4537l1klr.cloudfront.net/raschka/v-5/Figures/03__image035.png\">"
      ],
      "metadata": {
        "id": "VsJMLbiVKf1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos melhorar o SelfAttention_v1implementação adicional utilizando PyTorch's nn.Linearcamadas, que realizam efetivamente a multiplicação de matrizes quando as unidades de polarização estão desativadas. Além disso, uma vantagem significativa de usar nn.Linearem vez de implementar manualmente nn.Parameter(torch.rand(...))é aquele nn.Linearpossui um esquema de inicialização de peso otimizado, contribuindo para um treinamento de modelo mais estável e eficaz."
      ],
      "metadata": {
        "id": "XGw04U0wKqKh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9UdvlPj5unYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classe de self-attention usando camadas lineares do pytorch\n",
        "\n",
        "class SelfAttention_v2(nn.Module):\n",
        "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "        attn_scores = queries @ keys.T\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "W6AOV6KOInbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
        "print(sa_v2(inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgsJnj8ULmws",
        "outputId": "78974793-c3b6-45a5-c5a1-6fbae81981bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.5337, -0.1051],\n",
            "        [-0.5323, -0.1080],\n",
            "        [-0.5323, -0.1079],\n",
            "        [-0.5297, -0.1076],\n",
            "        [-0.5311, -0.1066],\n",
            "        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Escondendo palavras futuras com atenção causal\n",
        "\n",
        "<img src=\"https://drek4537l1klr.cloudfront.net/raschka/v-5/Figures/03__image037.png\">\n",
        "\n",
        "\n",
        "\n",
        "Uma forma de obter a matriz de pesos de atenção mascarada na atenção causal é aplicar a função softmax aos escores de atenção, zerando os elementos acima da diagonal e normalizando a matriz resultante.\n",
        "<img src=\"https://drek4537l1klr.cloudfront.net/raschka/v-5/Figures/03__image039.png\">"
      ],
      "metadata": {
        "id": "PxBS9tyGxolv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cLc14sOHnH8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculando os pesos de atenção usando a função softmax\n",
        "queries = sa_v2.W_query(inputs)\n",
        "keys = sa_v2.W_key(inputs)\n",
        "attn_scores = queries @ keys.T\n",
        "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "id": "qo3-csO4L3ZN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fba5842-c34f-4116-b628-57f98b22ecae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1717, 0.1762, 0.1761, 0.1555, 0.1627, 0.1579],\n",
            "        [0.1636, 0.1749, 0.1746, 0.1612, 0.1605, 0.1652],\n",
            "        [0.1637, 0.1749, 0.1746, 0.1611, 0.1606, 0.1651],\n",
            "        [0.1636, 0.1704, 0.1702, 0.1652, 0.1632, 0.1674],\n",
            "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.1639],\n",
            "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos implementar a etapa 2 usando a função `tril`do pytorch para criar uma mascara onde os valores acima da diagonal são zero"
      ],
      "metadata": {
        "id": "xNHcZx2c1X5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.tril() -> retorna a parte inferior diagonal da matriz, a parte superior é preenchida com zeros\n",
        "a = torch.randn(6,6)\n",
        "print(a)\n",
        "s = torch.tril(a)\n",
        "print(\"\\n\",s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdNZFNGJncTD",
        "outputId": "db9d900d-099d-4b20-cf65-4107e95e9cb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1690,  0.9178,  1.5810,  1.3010,  1.2753, -0.2010],\n",
            "        [-0.1606, -0.4015,  0.9666, -1.1481, -1.1589,  0.3255],\n",
            "        [-0.6315, -2.8400, -0.7849, -1.4096, -2.1338,  1.0524],\n",
            "        [-0.3885, -0.9343,  1.0533,  0.1388, -0.2044, -2.2685],\n",
            "        [-0.9133, -0.4204,  1.3111, -0.2199,  0.1838,  0.2293],\n",
            "        [ 0.6177, -0.2876,  0.8218,  0.1512,  0.1036, -2.1996]])\n",
            "\n",
            " tensor([[-0.1690,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.1606, -0.4015,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.6315, -2.8400, -0.7849,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.3885, -0.9343,  1.0533,  0.1388,  0.0000,  0.0000],\n",
            "        [-0.9133, -0.4204,  1.3111, -0.2199,  0.1838,  0.0000],\n",
            "        [ 0.6177, -0.2876,  0.8218,  0.1512,  0.1036, -2.1996]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = attn_scores.shape[0]\n",
        "mask_simple = torch.tril(torch.ones(block_size, block_size))\n",
        "print(mask_simple)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckjHYrU81M_j",
        "outputId": "5686d694-67a8-449b-d845-d4e36f66b389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiplicando esta mascara pelos attention weights (pesos de atenção) para zerar os valores acima da diagonal"
      ],
      "metadata": {
        "id": "Lmq1cXTB11v2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "masked_simple = attn_weights*mask_simple\n",
        "print(masked_simple)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Orukfm631gKy",
        "outputId": "2b23c2c0-0ce8-4b51-92f9-8ea269614920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1717, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1636, 0.1749, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1637, 0.1749, 0.1746, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1636, 0.1704, 0.1702, 0.1652, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.0000],\n",
            "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O terceiro passo é renormalizar os pesos de atenção (attention weights) para somar 1 novamente em cada linha"
      ],
      "metadata": {
        "id": "4INBp4gc2GRN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p_ReItzioaPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row_sums = masked_simple.sum(dim=1, keepdim=True) #dim=1 -> soma ao longo das dimensões das linhaas\n",
        "masked_simple_norm = masked_simple / row_sums\n",
        "print(masked_simple_norm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KO5N54QE1-0J",
        "outputId": "3850d1bc-ce03-47a3-811a-6757d9334d84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
            "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
            "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
            "       grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "row_sums"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "to5g4Jyf2YKB",
        "outputId": "d9d7e0cf-6d83-4a89-faa3-056f903b8299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1717],\n",
              "        [0.3385],\n",
              "        [0.5132],\n",
              "        [0.6693],\n",
              "        [0.8361],\n",
              "        [1.0000]], grad_fn=<SumBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Uma maneira mais eficiente de obter a matriz de pesos de atenção mascarada na atenção causal é mascarar os scores de atenção com valores infinitos negativos antes de aplicar a função softmax.**\n",
        "\n",
        "<img src=\"https://drek4537l1klr.cloudfront.net/raschka/v-5/Figures/03__image041.png\">\n",
        "\n",
        "A função softmax converte suas entradas em uma distribuição de probabilidade. Quando valores infinitos negativos (-∞) estão presentes em uma linha, a função softmax os trata como probabilidade zero. (Matematicamente, isso ocorre porque e - ∞ se aproxima de 0.)\n",
        "\n",
        "Podemos implementar esse \"truque\" de mascaramento mais eficiente criando uma máscara com 1's acima da diagonal e então substituindo esses 1's por infinito negativo ( -inf) valores:"
      ],
      "metadata": {
        "id": "gQ5vuN6p5KzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.triu retorna valores de 0 na diagonal inferior"
      ],
      "metadata": {
        "id": "nUIg48oBEDSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.triu(torch.ones(block_size, block_size), diagonal=1)\n",
        "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
        "print(masked)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrtmUL3x4ADx",
        "outputId": "acc94119-daba-46a7-a43d-0cd8e36f539a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3111,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.1655, 0.2602,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.1667, 0.2602, 0.2577,   -inf,   -inf,   -inf],\n",
            "        [0.0510, 0.1080, 0.1064, 0.0643,   -inf,   -inf],\n",
            "        [0.1415, 0.1875, 0.1863, 0.0987, 0.1121,   -inf],\n",
            "        [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8JlOCR-_pHrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora tudo o que precisamos fazer é aplicar a função softmax a esses resultados mascarados e pronto:"
      ],
      "metadata": {
        "id": "cn9nJVsM6o_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(masked / keys.shape[-1] ** 0.5, dim=1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XPGiW1b4MKB",
        "outputId": "3e55d32c-4d93-464d-c323-b11070afbd12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
            "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
            "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como podemos ver com base na saída, os valores em cada linha somam 1 e nenhuma normalização adicional é necessária\n",
        "\n",
        "Poderíamos agora usar os pesos de atenção modificados para calcular os vetores de contexto via context_vec = attn_weights @ values, como na seção 3.4. No entanto, na próxima seção, abordaremos primeiro outro pequeno ajuste no mecanismo de atenção causal que é útil para reduzir o overfitting ao treinar LLMs."
      ],
      "metadata": {
        "id": "ia4uujmw7EnU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Mascarando pesos de atenção adicionais com dropout\n",
        "\n",
        "<img src=\"https://drek4537l1klr.cloudfront.net/raschka/v-5/Figures/03__image043.png\">"
      ],
      "metadata": {
        "id": "_On_IM9m8MXM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7bLEDtTgEY9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "dropout = torch.nn.Dropout(0.5) #dropout de 50%\n",
        "example = torch.ones(6, 6) #matriz de 1's\n",
        "print(dropout(example))"
      ],
      "metadata": {
        "id": "l4n8xBno7B4g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22fe76dc-e3cf-4a3f-972d-f132c3d1bb02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2., 2., 2., 2., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.],\n",
            "        [0., 0., 2., 0., 2., 0.],\n",
            "        [2., 2., 0., 0., 0., 2.],\n",
            "        [2., 0., 0., 0., 0., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ao aplicar o dropout a uma matriz de peso de atenção com uma taxa de 50%, metade dos elementos da matriz são definidos aleatoriamente como zero.  Para compensar a redução nos elementos ativos, os valores dos elementos restantes na matriz são aumentados por um fator de 1/0,5 =2."
      ],
      "metadata": {
        "id": "vhTCGgW1vrQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# aplicando o dropout a matriz de peso\n",
        "torch.manual_seed(123)\n",
        "print(dropout(attn_weights))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vE6eczRzvXCN",
        "outputId": "4070b9e8-dfb0-4d9f-91de-3cf06260aca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 1.0335, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.6804, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4889, 0.5090, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3988, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.3418, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementando uma classe compacta de atenção causal\n",
        "\n",
        "Mas antes de começarmos, mais uma coisa é garantir que o código possa lidar com lotes que consistem em mais de uma entrada, para que o CausalAttentionclasse suporta as saídas em lote produzidas pelo carregador de dados que implementamos no capítulo 2.\n",
        "\n",
        "Para simplificar, para simular tais entradas em lote, duplicamos o exemplo do texto de entrada:"
      ],
      "metadata": {
        "id": "vbvzlJ9rxKc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape)\n",
        "\n",
        "#2 entradas com 6 tokens cada, e cada token tem dimensão de incorporação 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAdIbKCvvcQF",
        "outputId": "1b497ab3-d8c0-4dc6-ded3-4fb478bd6ccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hmd31FfVrPUc",
        "outputId": "e65e5194-a2c6-4770-cbfb-f1529778c792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.4300, 0.1500, 0.8900],\n",
              "         [0.5500, 0.8700, 0.6600],\n",
              "         [0.5700, 0.8500, 0.6400],\n",
              "         [0.2200, 0.5800, 0.3300],\n",
              "         [0.7700, 0.2500, 0.1000],\n",
              "         [0.0500, 0.8000, 0.5500]],\n",
              "\n",
              "        [[0.4300, 0.1500, 0.8900],\n",
              "         [0.5500, 0.8700, 0.6600],\n",
              "         [0.5700, 0.8500, 0.6400],\n",
              "         [0.2200, 0.5800, 0.3300],\n",
              "         [0.7700, 0.2500, 0.1000],\n",
              "         [0.0500, 0.8000, 0.5500]]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hcEzVfV4xsfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Uma classe compacta de atenção causal\n",
        "\n",
        "class CausalAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, block_size, dropout, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "           'mask',\n",
        "           torch.triu(torch.ones(block_size, block_size),\n",
        "           diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "        #New batch dimension b\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(1, 2)\n",
        "        attn_scores.masked_fill_(\n",
        "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "XTzqdzzJyuJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "block_size = batch.shape[1]\n",
        "ca = CausalAttention(d_in, d_out, block_size, 0.0)\n",
        "context_vecs = ca(batch)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uvgzp2Ex0Ca6",
        "outputId": "1f623634-2c87-435b-9661-1232bfddd492"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context_vecs.shape: torch.Size([2, 6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estendendo a atenção de uma única cabeça para a atenção de múltiplas cabeças (multi-head)\n",
        "\n",
        "O módulo de atenção com múltiplas cabeças nesta figura representa dois módulos de atenção com uma única cabeça empilhados um sobre o outro. Portanto, em vez de usar uma única matriz W v para calcular as matrizes de valor, em um módulo de atenção multicabeças com duas cabeças, agora temos duas matrizes de peso de valor: W v1 e W v2 . O mesmo se aplica às outras matrizes de peso, W q e W k . Obtemos dois conjuntos de vetores de contexto Z 1 e Z 2 que podemos combinar em uma única matriz de vetor de contexto Z .\n",
        "\n",
        "<img src=\"https://drek4537l1klr.cloudfront.net/raschka/v-5/Figures/03__image047.png\">"
      ],
      "metadata": {
        "id": "WQlUDD1xCG9k"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kn2XoA1BBYrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uma classe wrapper para implementar atenção multi-head\n",
        "\n",
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "    def __init__(self, d_in, d_out, block_size,\n",
        "                 dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [CausalAttention(d_in, d_out, block_size, dropout, qkv_bias)\n",
        "             for _ in range(num_heads)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([head(x) for head in self.heads], dim=-1)\n"
      ],
      "metadata": {
        "id": "yMdIwO5L0DBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando o MultiHeadAttentionWrapper, especificamos o número de cabeças de atenção ( num_heads). Se definirmos num_heads=2, conforme mostrado nesta figura, obtemos um tensor com dois conjuntos de matrizes de vetores de contexto. Em cada matriz de vetor de contexto, as linhas representam os vetores de contexto correspondentes aos tokens e as colunas correspondem à dimensão de incorporação especificada via d_out=2. Concatenamos essas matrizes de vetores de contexto ao longo da dimensão da coluna. Como temos 2 cabeças de atenção e uma dimensão de incorporação de 4, a dimensão de incorporação final é 2 × 2 = 4.\n",
        "\n",
        "<img src=\"https://drek4537l1klr.cloudfront.net/raschka/v-5/Figures/03__image049.png\">"
      ],
      "metadata": {
        "id": "PDPgPx6VGyDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Usando a classe\n",
        "torch.manual_seed(123)\n",
        "block_size = batch.shape[1] # numero de tokens\n",
        "d_in, d_out = 3, 2\n",
        "\n",
        "mha = MultiHeadAttentionWrapper(d_in, d_out, block_size, 0.0, num_heads=2)\n",
        "\n",
        "context_vecs = mha(batch)\n",
        "\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRX428QaGnwJ",
        "outputId": "3ddff7dc-fb4f-4850-ec88-104e0fb6011d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.0844,  0.0414,  0.0766,  0.0171],\n",
            "         [-0.2264, -0.0039,  0.2143,  0.1185],\n",
            "         [-0.4163, -0.0564,  0.3878,  0.2453],\n",
            "         [-0.5014, -0.1011,  0.4992,  0.3401],\n",
            "         [-0.7754, -0.1867,  0.7387,  0.4868],\n",
            "         [-1.1632, -0.3303,  1.1224,  0.8460]],\n",
            "\n",
            "        [[-0.0844,  0.0414,  0.0766,  0.0171],\n",
            "         [-0.2264, -0.0039,  0.2143,  0.1185],\n",
            "         [-0.4163, -0.0564,  0.3878,  0.2453],\n",
            "         [-0.5014, -0.1011,  0.4992,  0.3401],\n",
            "         [-0.7754, -0.1867,  0.7387,  0.4868],\n",
            "         [-1.1632, -0.3303,  1.1224,  0.8460]]], grad_fn=<CatBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementando atenção multicabeça com divisão de peso\n",
        "\n",
        "Em vez de manter duas classes separadas, MultiHeadAttentionWrappere CausalAttention, podemos combinar esses dois conceitos em uma única classe MultiHeadAttention.\n",
        "\n",
        "\n",
        "\n",
        "o seguinte MultiHeadAttentionclass integra a funcionalidade multi-head em uma única classe. Ele divide a entrada em vários heads, remodelando a consulta projetada, a chave e os tensores de valor e, em seguida, combina os resultados desses heads após calcular a atenção."
      ],
      "metadata": {
        "id": "CC0BWz7GIHuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uma classe eficiente de multi-head attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out,\n",
        "                 block_size, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out) # combinar as saídas das heads\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            'mask',\n",
        "             torch.triu(torch.ones(block_size, block_size), diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(2, 3)\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "        mask_unsqueezed = mask_bool.unsqueeze(0).unsqueeze(0)\n",
        "        attn_scores.masked_fill_(mask_unsqueezed, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)\n",
        "        return context_vec\n",
        "\n"
      ],
      "metadata": {
        "id": "Q9xGYzrjG9N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " No MultiheadAttentionWrapperclasse com duas cabeças de atenção, inicializamos duas matrizes de peso W q1 e W q2 e calculamos duas matrizes de consulta Q 1 e Q 2 conforme ilustrado na parte superior desta figura. No MultiheadAttentionclasse, inicializamos uma matriz de peso maior W q , realizamos apenas uma multiplicação de matriz com as entradas para obter uma matriz de consulta Q e, em seguida, dividimos a matriz de consulta em Q 1 e Q 2 conforme mostrado na parte inferior desta figura. Fazemos o mesmo com as chaves e valores, que não são mostrados para reduzir a confusão visual.\n",
        "\n",
        " <img src=\"https://drek4537l1klr.cloudfront.net/raschka/v-5/Figures/03__image051.png\">"
      ],
      "metadata": {
        "id": "Nphfy9HRCuRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para ilustrar esta multiplicação de matrizes em lote, suponha que temos o seguinte exemplo de tensor:\n",
        "\n"
      ],
      "metadata": {
        "id": "JmXpR8YSDj04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
        "                    [0.8993, 0.0390, 0.9268, 0.7388],\n",
        "                    [0.7179, 0.7058, 0.9156, 0.4340]],\n",
        "\n",
        "                   [[0.0772, 0.3565, 0.1479, 0.5331],\n",
        "                    [0.4066, 0.2318, 0.4545, 0.9737],\n",
        "                    [0.4606, 0.5159, 0.4220, 0.5786]]]])"
      ],
      "metadata": {
        "id": "eEwsUbph_rmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a.shape # b, num_heads, num_tokens, head_dim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npf3y4L9Dpgt",
        "outputId": "b7de72cf-5045-4a7c-bd9e-2d3fdef2137a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 2, 3, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora, realizamos uma multiplicação de matrizes em lote entre o próprio tensor e uma visão do tensor onde transpomos as duas últimas dimensões, num_tokense head_dim:"
      ],
      "metadata": {
        "id": "b2zoIOtQD0ZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ahB-uL2-KpKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(a @ a.transpose(2, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s37RAkr0DqL9",
        "outputId": "0a64057c-3b62-412d-b64c-f017437864fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[1.3208, 1.1631, 1.2879],\n",
            "          [1.1631, 2.2150, 1.8424],\n",
            "          [1.2879, 1.8424, 2.0402]],\n",
            "\n",
            "         [[0.4391, 0.7003, 0.5903],\n",
            "          [0.7003, 1.3737, 1.0620],\n",
            "          [0.5903, 1.0620, 0.9912]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_head = a[0, 0, :, :]\n",
        "first_res = first_head @ first_head.T\n",
        "print(\"First head:\\n\", first_res)\n",
        "\n",
        "second_head = a[0, 1, :, :]\n",
        "second_res = second_head @ second_head.T\n",
        "print(\"\\nSecond head:\\n\", second_res)"
      ],
      "metadata": {
        "id": "tkkn1ydeD3qt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9df748c4-5259-4b1d-8d88-c9633caf9ca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First head:\n",
            " tensor([[1.3208, 1.1631, 1.2879],\n",
            "        [1.1631, 2.2150, 1.8424],\n",
            "        [1.2879, 1.8424, 2.0402]])\n",
            "\n",
            "Second head:\n",
            " tensor([[0.4391, 0.7003, 0.5903],\n",
            "        [0.7003, 1.3737, 1.0620],\n",
            "        [0.5903, 1.0620, 0.9912]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Usando essa bodega\n",
        "\n",
        "torch.manual_seed(123)\n",
        "batch_size, block_size, d_in = batch.shape\n",
        "d_out = 2\n",
        "mha = MultiHeadAttention(d_in, d_out, block_size, 0.0, num_heads=2)\n",
        "context_vecs = mha(batch)\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBiAuv16LFWG",
        "outputId": "228f9857-644b-48a3-845f-0c08f92d622d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.3190, 0.4858],\n",
            "         [0.2943, 0.3897],\n",
            "         [0.2856, 0.3593],\n",
            "         [0.2693, 0.3873],\n",
            "         [0.2639, 0.3928],\n",
            "         [0.2575, 0.4028]],\n",
            "\n",
            "        [[0.3190, 0.4858],\n",
            "         [0.2943, 0.3897],\n",
            "         [0.2856, 0.3593],\n",
            "         [0.2693, 0.3873],\n",
            "         [0.2639, 0.3928],\n",
            "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **register_buffer()**\n",
        "\n",
        "Em PyTorch, buffers são tensores que são armazenados como parte do estado de um módulo, mas não são atualizados durante o treinamento. Isso significa que eles não são otimizados pelo otimizador e seus valores não mudam durante a propagação para trás. Buffers são úteis para armazenar dados que precisam ser persistidos com o modelo, mas que não precisam ser atualizados durante o treinamento.\n",
        "\n",
        "**Diferença entre Buffers e Parâmetros**\n",
        "\n",
        "A principal diferença entre buffers e parâmetros é que os parâmetros são atualizados durante o treinamento, enquanto os buffers não. Parâmetros são tipicamente os pesos de uma rede neural, enquanto buffers podem ser usados para armazenar outros tipos de dados, como contagens de iteração, estatísticas de normalização ou embeddings."
      ],
      "metadata": {
        "id": "lCrwGHqbt0Zm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class MyModule(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.register_buffer(\"counter\", torch.ones(2, 3))\n",
        "\n",
        "module = MyModule()\n",
        "print(module.counter)"
      ],
      "metadata": {
        "id": "1gv-_vd7LfT9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34a87816-1ccf-4ce1-fb18-6847907fc588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModule(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.register_buffer('counter', torch.ones(6,6))\n",
        "\n",
        "module = MyModule()\n",
        "print(module.counter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJV51vwV2OJq",
        "outputId": "0cdc07c0-36cc-4ba9-cde1-a14479240080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **torch.view()**\n",
        "\n",
        "O método .view() em PyTorch é usado para alterar a forma de um tensor sem copiar seus dados subjacentes. Isso significa que o tensor original e o tensor retornado pelo .view() compartilham os mesmos dados. O método .view() é uma ferramenta poderosa que pode ser usada para realizar uma variedade de operações em tensores, como transposição, achatamento e reshape."
      ],
      "metadata": {
        "id": "d83afuha3Som"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# achatamento\n",
        "\n",
        "x = torch.tensor([[1,2,3], [4,5,6]])\n",
        "print(x)\n",
        "\n",
        "y = x.view(3,2)\n",
        "print('\\n',y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvdEGRXo0AeD",
        "outputId": "988b1a65-c3e3-4fef-8276-150c7d24a579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "\n",
            " tensor([[1, 2],\n",
            "        [3, 4],\n",
            "        [5, 6]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Redimensionamento com Inferência de dimensões\n",
        "\n",
        "x = torch.arange(10)\n",
        "print(x)\n",
        "\n",
        "x_reshaped = x.view(-1, 5)\n",
        "print(\"\\n\",x_reshaped)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9855CTn0CBK",
        "outputId": "0840aa25-7219-4c3c-975d-b7a9f699fd7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
            "\n",
            " tensor([[0, 1, 2, 3, 4],\n",
            "        [5, 6, 7, 8, 9]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# achatamento\n",
        "\n",
        "x = torch.tensor([[1,2,3], [4,5,6]])\n",
        "print(x)\n",
        "\n",
        "y = x.view(-1)\n",
        "print(\"\\n\", y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GG1KUDTq8OAJ",
        "outputId": "1857aecc-54e9-484f-da51-d385d53c6b67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "\n",
            " tensor([1, 2, 3, 4, 5, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j9Pxd9-u8sqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k2A8SkU087PJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}