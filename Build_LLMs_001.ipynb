{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Trabalhando com dados de texto"
      ],
      "metadata": {
        "id": "WCYDJHL4bFwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenização de texto**\n",
        "\n",
        "O texto que iremos tokenizar para o treinamento LLM é um conto de Edith Wharton chamado The Verdict"
      ],
      "metadata": {
        "id": "PzQK84uyU2_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O the-verdict.txt \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucT8Z9tvh85z",
        "outputId": "26774a0d-2bc3-4134-be91-76909f95ec49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-05 16:37:12--  https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20479 (20K) [text/plain]\n",
            "Saving to: ‘the-verdict.txt’\n",
            "\n",
            "the-verdict.txt     100%[===================>]  20.00K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-05 16:37:12 (94.6 MB/s) - ‘the-verdict.txt’ saved [20479/20479]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mYrvqeZRiXR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lendo um conto como exemplo de texto em python\n",
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read() # lê o arquivo de texto e o armazena na var raw_text\n",
        "\n",
        "print(\"Total number of character: \", len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY4MXTB6iCY6",
        "outputId": "cddc1273-ad25-49a1-accf-de087bad1282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character:  20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0jBoD6rrifCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GOAL: Transformar esses 20479 caracteres em palavras individuas e caracteres individuais"
      ],
      "metadata": {
        "id": "-K7hOwsbWJdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "rZ3nFwCPVfgE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2883dda9-17f6-48a7-d7f3-17e6492153d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# separando pontos e virgulas\n",
        "result = re.split(r'([,.]|\\s)', text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZbpJTNbLNK9",
        "outputId": "b48ab2c1-99e1-417b-9441-eae9ecbf4d67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = [item.strip() for item in result if item.strip()] # remove qualquer espaço em branco, se o item tiver espaço em branco\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCiA2_i0LQHT",
        "outputId": "badcd0f9-e807-4b51-f6ee-9d9f8eaaba19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, world. Is this-- a test?\"\n",
        "result = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IC-kgPBLRhK",
        "outputId": "3b3de870-04fd-442b-b506-2bb38271f010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lq84Y4xkWVUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "basicamente o que fizemos:\n",
        "\n",
        "<img src=\"https://drek4537l1klr.cloudfront.net/raschka/v-3/Figures/02__image009.png\">"
      ],
      "metadata": {
        "id": "QiGkXs2iLV73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# aplicando em todo o conto de Edith Wharton:\n",
        "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(len(preprocessed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TETRbHn7jUvp",
        "outputId": "f50f612f-2a1e-489b-ecb0-d8e15f1274d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(preprocessed[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS2yeWa9jgE3",
        "outputId": "05b29beb-8058-489b-ce4b-91a3a36ab13a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Convertendo tokens em IDs de token**\n",
        "\n",
        "<img src=\"https://drek4537l1klr.cloudfront.net/raschka/v-3/Figures/02__image011.png\">"
      ],
      "metadata": {
        "id": "6AeYICJyLglD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# criando uma lista de tokens exclusivos\n",
        "all_words = sorted(list(set(preprocessed))) # sorted = organiza por ordem alfabetica, set = conjunto, remove as duplicatas\n",
        "vocab_size = len(all_words)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "hiWRN905H9xm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ab5de80-00d9-4cc6-b085-8ad6eb5df664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h-0WCRANk4Ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# criando um vocabulario:\n",
        "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
        "for i,item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i > 50:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfnvhwY-Wwl4",
        "outputId": "a9247d8e-6baf-4a01-c10b-05069b888d1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Carlo;', 25)\n",
            "('Chicago', 26)\n",
            "('Claude', 27)\n",
            "('Come', 28)\n",
            "('Croft', 29)\n",
            "('Destroyed', 30)\n",
            "('Devonshire', 31)\n",
            "('Don', 32)\n",
            "('Dubarry', 33)\n",
            "('Emperors', 34)\n",
            "('Florence', 35)\n",
            "('For', 36)\n",
            "('Gallery', 37)\n",
            "('Gideon', 38)\n",
            "('Gisburn', 39)\n",
            "('Gisburns', 40)\n",
            "('Grafton', 41)\n",
            "('Greek', 42)\n",
            "('Grindle', 43)\n",
            "('Grindle:', 44)\n",
            "('Grindles', 45)\n",
            "('HAD', 46)\n",
            "('Had', 47)\n",
            "('Hang', 48)\n",
            "('Has', 49)\n",
            "('He', 50)\n",
            "('Her', 51)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vDvm86iuMrL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "51DqUiIeM53D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos instanciar um novo objeto tokenizer"
      ],
      "metadata": {
        "id": "ZRV1AOn7sKoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFio0E2Kn2AR",
        "outputId": "0b538a59-39d8-456a-809b-a4d32f08aad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 58, 2, 872, 1013, 615, 541, 763, 5, 1155, 608, 5, 1, 69, 7, 39, 873, 1136, 773, 812, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "XSQq9yfDsXOi",
        "outputId": "cb5360c9-5fe6-4de4-9ca7-2c3e9b90ad92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok ele funcionou bem, vamos agora aplica-lo a um novo exemplo de texto que não está contido no conjunto de treinamento:\n"
      ],
      "metadata": {
        "id": "ciIMzhMtsh6S"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bzy9mgW3oO-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# esse código dará erro:\n",
        "#text = \"Hello, do you like tea?\"\n",
        "#tokenizer.encode(text)"
      ],
      "metadata": {
        "id": "0oWJWOA8sa1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O problema é que a palavra \"hello\" não foi usada no The Verdict conto . Portanto, não está contido no vocabulário. Isto destaca a necessidade de considerar conjuntos de formação grandes e diversificados para ampliar o vocabulário ao trabalhar em LLMs."
      ],
      "metadata": {
        "id": "it9d2BW8sv6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Adicionando tokens de contexto especiais**\n",
        "\n",
        "UNK E ENDOFTEXT\n",
        "\n",
        "Vamos agora modificar o vocabulário para incluir esses dois tokens especiais, <unk>e <|endoftext|>, adicionando-as à lista de todas as palavras exclusivas que criamos na seção anterior:"
      ],
      "metadata": {
        "id": "1ZmHI57qs9vC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
        "\n",
        "print(len(vocab.items()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhFksALgout_",
        "outputId": "3a3879fb-09b9-4e4a-b4e3-fc3a4a282b80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vamos imprimir as ultimas 5 entradas do vocabulario atualizado para conferir\n",
        "for i,item in enumerate(list(vocab.items())[-5:]):\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5w1X3_yo_AG",
        "outputId": "673c31d5-bdbc-4776-d386-4c27c40b7ef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('younger', 1156)\n",
            "('your', 1157)\n",
            "('yourself', 1158)\n",
            "('<|endoftext|>', 1159)\n",
            "('<|unk|>', 1160)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Um tokenizador de texto simples que lida com palavras desconhecidas\n",
        "\n",
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [item if item in self.str_to_int\n",
        "                        else \"<|unk|>\" for item in preprocessed]\n",
        "\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "wjpQ3agXxYLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vamos testar essa bosta agora\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NF_x9TKayUMJ",
        "outputId": "e25afdf2-a870-44c6-aefe-6b49dd7b75d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "print(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChedpNR9ytgI",
        "outputId": "7d47c868-bfc9-483f-c12c-0eb9ee3bbabc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1160, 5, 362, 1155, 642, 1000, 10, 1159, 57, 1013, 981, 1009, 738, 1013, 1160, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1159 -> <|endoftext|>\n",
        "\n",
        "1160 -> <|UNK|>"
      ],
      "metadata": {
        "id": "9D6zvRdgzMUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# destokenização QUE PALAVRA DIFICIL BRO\n",
        "print(tokenizer.decode(tokenizer.encode(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lnu8CfdOy12J",
        "outputId": "2e1f3c12-9107-49ba-d8d2-9430b10cd63b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Byte pair encoding"
      ],
      "metadata": {
        "id": "WkGRuoclORpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# algoritmo de BPE baseado em rust\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "nzZT0EwHzWjh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43edc574-c189-46da-bb06-a3dca87e0192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.8 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RF9G9DTPiCH",
        "outputId": "c800ed84-17f2-4841-feb1-21e2475004d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciando o tokenizer BPE do tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "Zi62CI6cPsRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esse tokenizer é semelhante ao `SimpleTokenizerV2`"
      ],
      "metadata": {
        "id": "plKwkR5UP2Me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "print(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EklFFZ_8Pz4v",
        "outputId": "5b804a23-5ef3-451e-b646-2b2e28f96410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV7lIbSxQKtx",
        "outputId": "774a86d4-7bdd-40d2-e0ea-69a1129a0e37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testando\n",
        "texto = \"Akwirw ier\"\n",
        "test = tokenizer.encode(texto)\n",
        "print(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp0bOL9HQP9_",
        "outputId": "5768e1c7-4fda-46e3-cb54-0349bb585a63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33901, 86, 343, 86, 220, 959]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c_decode = tokenizer.decode(test)\n",
        "c_decode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "e_Tn1LYcRgJZ",
        "outputId": "d2366996-4016-4bf1-f6b9-897cc3070a58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Akwirw ier'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Amostragem de dados com janela deslizante**\n",
        "\n",
        "Dada uma amostra de texto, extraia blocos de entrada como subamostras que servem como entrada para o LLM, e a tarefa de previsão do LLM durante o treinamento é prever a próxima palavra que segue o bloco de entrada. Durante o treinamento, mascaramos todas as palavras que ultrapassam o alvo. Observe que o texto mostrado nesta figura passaria por tokenização antes que o LLM pudesse processá-lo; no entanto, esta figura omite a etapa de tokenização para maior clareza.\n",
        "\n",
        "\n",
        "<img src=\"https://drek4537l1klr.cloudfront.net/raschka/v-3/Figures/02__image023.png\">\n"
      ],
      "metadata": {
        "id": "0BmG3w8cXnG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtpkwXDQqbjI",
        "outputId": "739280e3-d603-48b4-a614-bead76afc73f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5145 tokens após a aplicação do tokenizer BPE"
      ],
      "metadata": {
        "id": "qQ7hviNJYcOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#oi"
      ],
      "metadata": {
        "id": "sf6EVYWAzQrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rmeovendo os 50 primeiros tokens pq sim\n",
        "enc_sample = enc_text[50:]\n"
      ],
      "metadata": {
        "id": "0k67JsRXYZ22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uma das maneiras mais fáceis e intuitivas de criar pares de entrada-alvo para a tarefa de previsão da próxima palavra é criar duas variáveis, xe y, onde xcontém os tokens de entrada e ycontém os alvos, que são as entradas **deslocadas** em 1:"
      ],
      "metadata": {
        "id": "mvay5MFlZHr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# O tamanho do contexto determina quantos tokens estão incluídos na entrada\n",
        "\n",
        "context_size = 4\n",
        "\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:      {y}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGUI-j2rYxxo",
        "outputId": "72bb7be9-0ad3-4c48-9805-6278a1ccc107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [290, 4920, 2241, 287]\n",
            "y:      [4920, 2241, 287, 257]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "    print(context, \"---->\", desired)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8dFSFUlZApX",
        "outputId": "1e53e06e-2b18-4194-f148-4185092d7271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[290] ----> 4920\n",
            "[290, 4920] ----> 2241\n",
            "[290, 4920, 2241] ----> 287\n",
            "[290, 4920, 2241, 287] ----> 257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para fins de ilustração, vamos repetir o código anterior, mas converter os IDs dos tokens em texto:"
      ],
      "metadata": {
        "id": "MDv3s_wBZj5u"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0jEyfD9TftCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "  context = enc_sample[:i]\n",
        "  desired = enc_sample[i]\n",
        "  print(tokenizer.decode(context), \"----->\", tokenizer.decode([desired]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yZTdfX2Y2nl",
        "outputId": "562048e3-8989-474f-e37f-d11b161efd3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " and ----->  established\n",
            " and established ----->  himself\n",
            " and established himself ----->  in\n",
            " and established himself in ----->  a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Há apenas mais uma tarefa antes de podermos transformar os tokens em embeddings, como mencionamos no início deste capítulo: implementar um carregador de dados eficiente que itere sobre o conjunto de dados de entrada e retorne as entradas e os alvos como tensores PyTorch.\n",
        "\n",
        "\n",
        "Para implementar carregadores de dados eficientes, coletamos as entradas em um tensor, x, onde cada linha representa um contexto de entrada. Um segundo tensor, y, contém os alvos de previsão correspondentes (próximas palavras), que são criados mudando a entrada em uma posição.\n",
        "\n",
        "<img src=\"https://drek4537l1klr.cloudfront.net/raschka/v-3/Figures/02__image025.png\">"
      ],
      "metadata": {
        "id": "gLGWzHiXem7p"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Ff9VU3skM1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        token_ids = tokenizer.encode(txt) # tokenize o texto todo\n",
        "\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self): # retorna a quantidade total de linhas no conjunto de dados\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx): # retorna apenas uma linha no conjunto de dados.\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "exVY3FFtbaXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kmRzPSFOtEdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Um carregador de dados (dataloader) para gerar lotes com pares entrada-alvo\n",
        "def create_dataloader(txt, batch_size=4,\n",
        "        max_length=256, stride=128, shuffle=True):\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\") # inicializa o tokenizer\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) # cria o dataset\n",
        "    dataloader = DataLoader( # cria o dataloader\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "    return dataloader\n"
      ],
      "metadata": {
        "id": "raYIRpuigZaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos testar o dataloadercom tamanho de lote 1 para um LLM com tamanho de contexto 4 para desenvolver uma intuição de como o GPTDatasetV1classe da listagem 2.5 e a create_dataloaderfunção da listagem 2.6 funcionam juntas:"
      ],
      "metadata": {
        "id": "jLzMU9WZaIJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "dataloader = create_dataloader(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4c7WnMXZRi1",
        "outputId": "82d1ac94-1e81-48bd-f552-dbc7543d810e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para ilustrar o significado de stride=1, vamos buscar outro lote deste conjunto de dados:"
      ],
      "metadata": {
        "id": "VQ6gFZWDcV7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "stride tem a ver com deslocamento"
      ],
      "metadata": {
        "id": "h4YSjRvXdLLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5u_fWlsbSl0",
        "outputId": "a0820121-efc9-4ef3-8ca8-35d62488b0b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se compararmos o primeiro com o segundo batch (lote), podemos ver que os IDs de token do segundo lote saão **deslocados** em uma posição em comparação com o primeiro lote (por exemplo, o segundo ID na entrada do primeiro lote é 367, que é o primeiro ID de entrada do segundo lote). A configuração `Stride`  determina o número de posições que as entradas mudam entre lotes, emulando uma abordagem de janela deslizante."
      ],
      "metadata": {
        "id": "OxpivlZ7c_eK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "third_batch = next(data_iter)\n",
        "print(third_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCfj9nYAcaXE",
        "outputId": "0bf062d9-548b-4995-a854-393b985b9845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[2885, 1464, 1807, 3619]]), tensor([[1464, 1807, 3619,  402]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ao criar vários lotes a partir do conjunto de dados de entrada, deslizamos uma janela de entrada pelo texto. Se o passo for definido como 1, deslocaremos a janela de entrada em 1 posição ao criar o próximo lote. Se definirmos o passo igual ao tamanho da janela de entrada, podemos evitar sobreposições entre os lotes.\n",
        "\n",
        "<img src=\"https://drek4537l1klr.cloudfront.net/raschka/v-3/Figures/02__image027.png\">"
      ],
      "metadata": {
        "id": "ULGLZ9m6excK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "vamos dar uma breve olhada em como podemos usar o carregador de dados para fazer amostras com um tamanho de lote maior que 1:"
      ],
      "metadata": {
        "id": "Q2R_LdBPfc78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader(raw_text, batch_size=8, max_length=4, stride=5)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVO7nNaRd-3f",
        "outputId": "f1c2d985-8830-4309-d285-a3ba5bc3d71a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[  607,   599,  6321,   287],\n",
            "        [  465, 11441, 48740,    11],\n",
            "        [  520,  5493,  2241,   318],\n",
            "        [  287,  1936,  2431,   438],\n",
            "        [   11,   465, 10904,  4252],\n",
            "        [ 1576,   284,   766,   465],\n",
            "        [15910,    13,   887,   484],\n",
            "        [  262, 37090,   257,   845]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  599,  6321,   287,   262],\n",
            "        [11441, 48740,    11,   345],\n",
            "        [ 5493,  2241,   318,   262],\n",
            "        [ 1936,  2431,   438,   392],\n",
            "        [  465, 10904,  4252,  6236],\n",
            "        [  284,   766,   465,  5986],\n",
            "        [   13,   887,   484,  4054],\n",
            "        [37090,   257,   845, 22665]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Criando incorporações(embedding) de token**\n",
        "\n",
        "<img src=\"https://drek4537l1klr.cloudfront.net/raschka/v-3/Figures/02__image029.png\">\n"
      ],
      "metadata": {
        "id": "8wLBfmo8gXwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# suponha que temos os seguintes quatro tokens de entrada com IDs 5,1,3 e 2\n",
        "input_ids = torch.tensor([5,1,3,4])"
      ],
      "metadata": {
        "id": "aioq-9myfwid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para fins de simplicidade e ilustração, suponha que temos um pequeno vocabulário de apenas 6 palavras (em vez das 50.257 palavras no vocabulário do tokenizer BPE) e queremos criar embeddings de tamanho 3 (no GPT-3, o tamanho de incorporação tem 12.288 dimensões):"
      ],
      "metadata": {
        "id": "XzBD7uyfl7Q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 6\n",
        "output_dim = 3"
      ],
      "metadata": {
        "id": "bQiAM8wKlxqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciando uma camada de Incorporação (embedding) though pytorch\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "print(embedding_layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxEY8EqymTIc",
        "outputId": "77f71be6-798c-4abd-878c-4ea5bff47458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A instrução print no exemplo de código anterior imprime a matriz de peso subjacente da camada de incorporação"
      ],
      "metadata": {
        "id": "UqscSqX8mmVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vamos aplicar a um ID de token para obter o vetor de embedding\n",
        "print(embedding_layer(torch.tensor([3])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iZ7SME8mgn8",
        "outputId": "321d9d97-6048-49d9-e886-e6997890a0ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se compararmos o vetor de incorporação do token ID 3 com a matriz de incorporação anterior, veremos que ele é idêntico à 4ª linha (Python começa com um índice zero, portanto é a linha correspondente ao índice 3). Em outras palavras, a camada de incorporação é essencialmente uma operação de pesquisa que recupera linhas da matriz de pesos da camada de incorporação por meio de um ID de token."
      ],
      "metadata": {
        "id": "1Ctl76zZm4fJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vamos agora aplicar a todos os IDs de entradas que definimos anteriormente\n",
        "print(embedding_layer(input_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAXmGikLmw9s",
        "outputId": "f42de689-b8d6-4ecb-f98b-b0948cac9213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-2.8400, -0.7849, -1.4096],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As camadas de incorporação realizam uma operação de pesquisa, recuperando o vetor de incorporação correspondente ao ID do token da matriz de pesos da camada de incorporação. Por exemplo, o vetor de incorporação do token ID 5 é a sexta linha da matriz de peso da camada de incorporação (é a sexta em vez da quinta linha porque o Python começa a contar em 0).\n",
        "\n",
        "<img src=\"https://drek4537l1klr.cloudfront.net/raschka/v-3/Figures/02__image031.png\">"
      ],
      "metadata": {
        "id": "wy6gI2qNnQC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Codificando posições de palavras**\n",
        "\n",
        "Embeddings posicionais são adicionados ao vetor de incorporação de token para criar os embeddings de entrada para um LLM. Os vetores posicionais têm a mesma dimensão que os embeddings de tokens originais. Os embeddings de token são mostrados com valor 1 para simplificar.\n",
        "\n",
        "<img src=\"https://drek4537l1klr.cloudfront.net/raschka/v-4/Figures/02__image035.png\">"
      ],
      "metadata": {
        "id": "SGlI4Ft5biD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dim = 256\n",
        "vocab_size = 50257\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "9kGmTkopnA6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# basicamente essa bosta terá 50257 linhas com 256 colunas cada"
      ],
      "metadata": {
        "id": "LjErvlSh1mdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# exemplo de uma linha na camada embedding:\n",
        "token_embedding_layer.weight[0]"
      ],
      "metadata": {
        "id": "AsZVXBbegXOS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13c48303-cb58-408c-9af1-4915d257941d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-2.1338e+00,  1.0524e+00, -3.8848e-01, -9.3435e-01, -4.9914e-01,\n",
              "        -1.0867e+00,  9.6242e-01,  2.4921e-01,  6.2662e-01, -1.7549e-01,\n",
              "         9.8284e-02, -9.3507e-02,  2.6621e-01, -5.8504e-01, -3.4304e-01,\n",
              "        -6.8215e-01, -1.4779e+00,  1.1331e+00, -1.2203e+00,  1.3139e+00,\n",
              "         1.0533e+00,  1.3881e-01, -2.0444e-01, -2.2685e+00, -2.8084e-01,\n",
              "         7.6968e-01, -6.5956e-01, -7.9793e-01,  1.8383e-01,  2.2935e-01,\n",
              "         6.1774e-01, -2.8758e-01, -2.5873e-01, -1.0826e+00, -4.4382e-02,\n",
              "         1.6236e+00, -2.3229e+00,  1.0878e+00, -6.3545e-02, -4.4864e-01,\n",
              "        -9.4872e-01, -7.6507e-02, -1.5264e-01,  1.1674e-01,  4.4026e-01,\n",
              "        -1.4465e+00, -5.5808e-01, -5.1696e-02,  1.0042e+00,  8.2723e-01,\n",
              "        -3.9481e-01,  4.8923e-01, -2.1681e-01, -1.7472e+00,  1.7228e+00,\n",
              "         7.7381e-01,  9.0315e-01, -7.2184e-01, -5.9508e-01, -7.1122e-01,\n",
              "         6.2296e-01, -1.3729e+00, -1.2806e-01, -1.2838e+00, -2.0915e+00,\n",
              "         9.6285e-01, -3.1861e-02, -4.7896e-01,  7.6681e-01,  2.7468e-02,\n",
              "         4.7470e-02, -9.2387e-01, -5.0087e-01, -2.7928e-01, -2.0628e+00,\n",
              "         6.3745e-03, -9.8955e-01,  7.0161e-01, -9.8224e-01,  2.7703e-01,\n",
              "         1.0322e+00, -2.8300e-01,  4.9275e-01, -1.4078e-02, -2.7466e-01,\n",
              "        -7.6409e-01, -5.8716e-01,  1.1952e+00, -1.5822e-03,  1.2471e+00,\n",
              "        -7.7105e-02,  1.2774e+00, -1.4596e+00, -2.1595e+00, -7.0671e-01,\n",
              "        -9.2224e-01, -8.0156e-01, -8.1830e-01, -1.1820e+00, -2.8774e-01,\n",
              "        -6.0430e-01,  6.0024e-01, -1.4205e+00, -2.2383e-01, -2.5479e-01,\n",
              "         1.1517e+00, -1.7858e-02,  4.2640e-01, -7.6574e-01, -5.4514e-02,\n",
              "        -7.3205e-01,  1.2347e+00, -2.2801e-01,  9.2238e-01,  2.0561e-01,\n",
              "        -4.9696e-01,  5.8206e-01,  2.0532e-01, -9.8437e-01,  1.9184e+00,\n",
              "        -6.1710e-01, -8.3339e-01,  4.8387e-01, -1.3493e-01,  2.1187e-01,\n",
              "        -8.7140e-01, -7.5571e-01, -1.9830e-01, -5.4688e-01,  1.6014e+00,\n",
              "        -2.2577e+00, -1.8009e+00,  7.0147e-01,  5.7028e-01,  1.8790e+00,\n",
              "        -9.1925e-01,  1.1318e-01,  1.4353e+00,  8.8307e-02, -1.2037e+00,\n",
              "         1.0964e+00,  2.4210e+00,  2.4489e-01,  1.8118e+00,  5.5035e-01,\n",
              "         6.5788e-02,  6.8050e-01,  1.2064e+00,  1.6250e+00,  3.4595e-01,\n",
              "        -1.8524e-01,  7.3711e-01,  2.2760e+00, -1.3255e+00, -8.9702e-01,\n",
              "         1.1318e-01,  8.3647e-01,  2.8520e-02, -4.5632e-01, -5.1855e-01,\n",
              "        -2.7214e-01, -3.5100e-01,  1.1152e+00, -6.1722e-01, -2.2708e+00,\n",
              "        -1.3819e+00, -8.4840e-01,  5.3230e-01, -4.0527e-01,  7.0864e-01,\n",
              "         9.5331e-01, -1.3035e-02, -1.3009e-01, -8.7660e-02,  4.1871e-01,\n",
              "        -1.1123e+00, -9.3917e-01, -1.0448e+00,  1.2783e+00,  4.1903e-01,\n",
              "        -5.0727e-01, -6.0623e-01,  4.7708e-01,  7.2026e-01, -1.0954e-01,\n",
              "        -3.3161e-01,  9.0084e-01,  4.8398e-01, -1.3237e+00,  7.8692e-01,\n",
              "        -1.5701e+00,  2.7477e-02, -7.6117e-01,  2.4163e-01, -5.8781e-01,\n",
              "        -1.1506e+00,  1.0164e+00,  1.2343e-01,  5.2596e-02, -1.1892e+00,\n",
              "        -5.9727e-02,  3.5527e-01, -1.4355e+00,  7.2748e-02,  1.0528e-01,\n",
              "        -1.0311e+00, -4.7796e-01,  7.9950e-01,  2.1181e-01, -8.6248e-03,\n",
              "         1.8576e+00,  2.1321e+00, -5.0561e-01, -7.9884e-01, -1.8127e+00,\n",
              "         1.1846e+00, -5.3986e-01,  1.2117e+00, -8.6321e-01,  1.3337e+00,\n",
              "         7.7101e-02, -5.2181e-02, -1.0565e+00,  1.1510e+00, -1.3354e+00,\n",
              "        -2.9340e+00,  1.1411e-01, -1.2072e+00, -3.0083e-01,  1.4274e-01,\n",
              "        -9.5917e-01, -1.2213e+00, -2.1429e+00,  9.4881e-01, -5.6842e-01,\n",
              "        -6.4643e-02,  6.6467e-01, -2.7836e+00, -1.4483e+00,  1.2933e+00,\n",
              "         9.4943e-01,  2.6565e-02, -9.2207e-01,  7.0338e-01, -3.6590e-01,\n",
              "        -1.9654e-01,  1.7250e+00, -9.3351e-02, -2.1734e-02,  3.4414e-01,\n",
              "         2.2710e-01, -4.5969e-01, -6.1831e-01,  2.4612e-01,  1.2119e+00,\n",
              "         3.1705e-01], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "\n",
        "dataloader = create_dataloader(raw_text, batch_size=8, max_length=max_length, stride=5, shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Token IDs: \\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEz9NCp41Dvz",
        "outputId": "a0ae8ea3-6a9f-4585-b124-affd300ae250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs: \n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [  257,  7026, 15632,   438],\n",
            "        [  257,   922,  5891,  1576],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [  326,    11,   287,   262],\n",
            "        [  286,   465, 13476,    11]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuW1oRVG2EzE",
        "outputId": "4324fe7e-e474-4d6d-ba71-d0fa95a16e4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(block_size, output_dim)\n",
        "pos_embedding = pos_embedding_layer(torch.arange(block_size))\n",
        "print(pos_embedding.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzyhzgv52QRE",
        "outputId": "884c5004-4aa0-4f1f-8929-8afefb011e11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + pos_embedding\n",
        "print(input_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LZ4galP2kOD",
        "outputId": "b62b344a-d17e-45be-d525-12f4cd28e3a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drek4537l1klr.cloudfront.net/raschka/v-4/Figures/02__image037.png\">"
      ],
      "metadata": {
        "id": "0GalbA7_20_v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1-rczOuj2wJL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}